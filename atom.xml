<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[OSv Blog]]></title>
  <link href="http://osv.io/blog/atom.xml" rel="self"/>
  <link href="http://osv.io/blog/"/>
  <updated>2014-06-23T07:59:29-07:00</updated>
  <id>http://osv.io/blog/</id>
  <author>
    <name><![CDATA[Cloudius Systems]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hypervisors Are Dead, Long Live the Hypervisor (Part 2)]]></title>
    <link href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/"/>
    <updated>2014-06-19T13:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2</id>
    <content type="html"><![CDATA[<p><strong>By Dor Laor and Avi Kivity</strong></p>

<h1>Linux containers</h1>

<p>(This is part 2 of a 3-part series. <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/">Part 1 was published yesterday.</a>)</p>

<p>Containers, which create isolated compartments at the operating system level instead of adding a separate hypervisor level, trace their history not to mainframe days, but to Unix systems.</p>

<p>FreeBSD introduced “jails” in 2000. There’s a good description of them in  <a href="http://phk.freebsd.dk/pubs/sane2000-jail.pdf">Jails: Confining the omnipotent root by Poul-Henning Kamp and Robert N. M. Watson</a>. Solaris got its Zones in 2005.  Both systems allowed for an isolated “root” user and root filesystem.</p>

<p>The containers we know today, <a href="https://linuxcontainers.org/">Linux Containers</a>, or LXC, are not a single monolithic system, but more of a concept, based on a combination of  several different isolation mechanisms built into Linux at the kernel level.  <a href="https://lwn.net/Articles/587545/">Linux Containers 1.0 was released earlier this year.</a>, but many of the underlying systems have been under development in Linux independently.  Containers are not an all-or-nothing design decision, and it’s possible for different systems to work with them in different ways. <a href="https://linuxcontainers.org/">LXC can use all of the following Linux features</a>:</p>

<ul>
<li><p>Kernel namespaces (ipc, uts, mount, pid, network and user)</p></li>
<li><p>AppArmor and SELinux profiles</p></li>
<li><p>Seccomp policies</p></li>
<li><p>Chroots (using pivot_root)</p></li>
<li><p>Kernel capabilities</p></li>
<li><p>Control groups (cgroups)</p></li>
</ul>


<p>Although the combination can be complex, there are tools that make containers simple to use. For several years userspace tools such as LXC, libvirt allowed users to manage containers. However, containers didn’t really get picked up by the masses until the creation of <a href="https://www.docker.io/">Docker</a>. Docker and <a href="http://man7.org/linux/man-pages/man1/systemd-nspawn.1.html">systemd-nspawn</a> can start containers with minimal configuration, or from the command line. The Docker developers deserve much credit for adding two powerful concepts above the underlying container complexity:
a. Public image repository &ndash; immediate search and download of containers pre-loaded with dependencies, and
b. Dead-simple execution &ndash; a one-liner command for running a container.</p>

<p><img src="http://osv.io/blog/images/docker.png" alt="Docker diagram" /></p>

<p><strong><a href="https://www.docker.io/the_whole_story/">Docker gives container users a simple build process and a public repository system.</a></strong></p>

<h2>Container advantages</h2>

<p>When deployed on a physical machine, containers can eliminate the need of running two operating systems, one on top of the other (as in traditional virtualization). It makes IO system calls almost native and the footprint is minimal. However, this comes with a cost as we will detail below. The rule of the thumb is that if you do not need multi-tenancy and you’re willing to do without a bunch of software defined features, containers on bare metal are perfect for you!</p>

<p>In production, <a href="https://speakerdeck.com/jbeda/containers-at-scale">Google uses containers extensively</a>, starting more than two billion per week. Each container includes an application, built together with its dependencies, and containerization helps the company manage diverse applications across many servers.</p>

<p>Containers are an excellent case for development and test. It becomes possible to test some fairly complex setups, such as a version control system with hooks, or an SMTP server with spam filters,  by running services in a container. Because a container can use namespaces to get a full set of port numbers, it’s easy to run multiple complex tests at a time. The systemd project even uses containers for testing their software, which manages an entire Linux system. Containers are highly useful for testing because of their fast startup time&mdash;you’re just invoking an isolated set of processes on an existing kernel, not booting an entire guest OS.</p>

<p>If you run multiple applications that depend on different versions of a dependency, then deploying each application within its own container can allow you to avoid dependency conflict problems. Containers in theory decouple the application from the operating system. We use the term &lsquo;in theory&rsquo; because lots of care and thought should be given to maintaining your stack. For example, will your container combo be supported by the host OS vendor? Is your container up-to-date and does it include fixes for bugs such as ‘heartbleed’? Is your host fully updated, and does its kernel API provide the capabilities your application requires?</p>

<p>We highly recommend the use of containers whenever your environment is homogeneous:</p>

<ul>
<li><p>No multitenancy</p></li>
<li><p>Application is always written with clustering in mind</p></li>
<li><p>Load balancing is achieved by killing running apps and re-spinning them elsewhere (as opposed to live migration)</p></li>
<li><p>No need to run different kernel versions</p></li>
<li><p>No underlying hypervisor (otherwise, you&rsquo;re just adding a layer)</p></li>
</ul>


<p>When the above apply, you will enjoy near bare-metal performance, a small footprint and fast boot time.</p>

<h2>Container disadvantages: Security</h2>

<p>It’s clear that a public cloud needs strong isolation separating tenant systems. All that an attacker needs is an email address and a credit card number to set up a hostile VM on the same hardware as yours. But strong isolation is also needed in private clouds behind the corporate firewall. Corporate IT won’t be keen to run <strong>sandbox42.interns.engr.example.com</strong> and <strong>payroll.example.com</strong> within the same security domain.</p>

<p>Hypervisors have a relatively simple security model. The interface between the guest and the hypervisor is well defined, based on real or virtual hardware specifications. Five decades of hypervisor development have helped to form a stable and mature interface. Large portions of the interface&rsquo;s security are enforced by the physical hardware.</p>

<p>Containers, on the other hand, are implemented purely in software. All containers and their host share the same kernel. Nearly the entire Linux kernel had to undergo changes in order to implement isolation for resources such as memory management, network stack, I/O, the scheduler, and user namespaces. The Linux community is investing a lot of effort to improve and expand container support. However, rapid development makes it harder to stabilize and harden the container interfaces.</p>

<h2>Container disadvantages: Software-defined data center</h2>

<p>Hypervisors are the basis for the new virtualized data center. They allow us to perfectly abstract the hardware and play nicely with networking and storage.
Today there isn&rsquo;t a switch or a storage system without VM integration or VM-specific features.</p>

<p>Can a virtualized data center be based on containers in place of hypervisors? At almost all companies, no.. There will always be security issues with mounting SAN devices and filesystems from containers in different security domains. Yes, containers are a good fit for plenty of tasks but are restricted when it comes to sensitive areas such as you data center building blocks such as the storage and the network.</p>

<p>No one operating system, even Linux, will run 100% of the applications in the data center. There will always be diversity at the data center, and the existence of different operating systems will force the enterprise to keep the abstraction at the VM level.</p>

<h2>Container disadvantages: Management</h2>

<p>The long history of hypervisors means that the industry has developed a huge collection of tools for real-world administration needs.</p>

<p><img src="http://osv.io/blog/images/esx2.jpg" alt="CPM monitoring in VMware" /></p>

<p><strong><a href="http://robertmorannet.blogspot.com/2010/08/vmware-vsphere-screenshots.html">Blogger Robert Moran shows a screenshot of CPU monitoring in VMware’s vSphere</a>.</strong></p>

<p>The underlying functionality for hypervisor management is also richer. All of the common hypervisors support “live migration” of guests from one host to another.</p>

<p>Hypervisors have become an essential tool in the community of practice around server administration. Corporate IT is in the process of virtualizing its diverse collection of servers, running modern and vintage Linux distributions, plus legacy operating systems, and hypervisor vendors including VMWare and Microsoft are enabling it.</p>

<h2>Container disadvantages: Complexity</h2>

<p>While containers take advantage of the power built into Linux, they share Linux’s complexity and diversity.  For example, each Linux distribution standardizes on a different kernel version, and some use AppArmor while others use SELinux. Because containers are implemented using multiple isolation features at the OS level, the “containerization” features can vary by kernel version and platform.</p>

<h2>The anatomy of a multi-tenant exploit</h2>

<p>Let&rsquo;s assume a cloud vendor, whether SaaS, IaaS, or PaaS, implements a service within a container. How would an attacker exploit it?
The first stage would be to gain control of the application within the container. Many applications have flaws and the attacker would need to exploit an existing unpatched CVE in order to gain access to the container. IaaS even makes it simpler as the attacker already has a “root” shell inside a neighboring container.</p>

<p>The next stage would be to penetrate the kernel. Unfortunately, the kernel&rsquo;s attack surface contains hundreds of system calls, and other vulnerabilities exist in the form of packets and file metadata that can jeopardize the kernel. Many attackers have access to zero-day exploits, unpublished local kernel vulnerabilities. (A typical “workflow” is to watch upstream kernel development for security-sensitive fixes, and figure out how to exploit them on the older kernels in production use.)</p>

<p>Once the hacker gains control of the kernel, it&rsquo;s game over. All the other tenants’ data is exposed.</p>

<p>The list of exploitable bugs is always changing, and there will probably be more available by the time you read this.  A few recent examples:</p>

<ul>
<li><p>“An information leak was discovered in the Linux kernel&rsquo;s SIOCWANDEV ioctl call. A local user with the CAP_NET_ADMIN capability could exploit this flaw to obtain potentially sensitive information from kernel memory.“ (CVE-2014-1444) Some container configurations have CAP_NET_ADMIN, while others don’t. Because it’s possible to set up containers in more or less restricted ways, individual sites need to check if they’re vulnerable. (Many LInux capabilities are <a href="http://forums.grsecurity.net/viewtopic.php?f=7&amp;t=2522">equivalent to root</a> because they can be used to obtain root access.)</p></li>
<li><p>“An information leak was discovered in the wanxl ioctl function in  Linux. A local user could exploit this flaw to obtain potentially sensitive information from kernel memory.” (CVE-2014-1445)”</p></li>
<li><p>“An unprivileged local user with access to a CIFS share could use this flaw to crash the system or leak kernel memory. Privilege escalation cannot be ruled out (since memory corruption is involved), but is unlikely.“ (CVE-2014-0069)</p></li>
</ul>


<p>Each individual vulnerability is usually fixed quickly, but there’s a constant flow of new ones for attackers to use. <a href="https://lwn.net/Articles/462756/">Linux filesystem developer Ted Ts’o wrote</a>,</p>

<blockquote><p>Something which is baked in my world view of containers (which I suspect is not shared by other people who are interested in using containers) is that given that the kernel is shared, trying to use containers to provide better security isolation between mutually suspicious users is hopeless.  That is, it&rsquo;s pretty much impossible to prevent a user from finding one or more zero day local privilege escalation bugs that will allow a user to break root.  And at that point, they will be able to penetrate the kernel, and from there, break security of other processes.</p>

<p>So if you want that kind of security isolation, you shouldn&rsquo;t be using containers in the first place.  You should be using KVM or Xen, and then only after spending a huge amount of effort fuzz testing the KVM/Xen paravirtualization interfaces.</p></blockquote>

<p><a href="http://permalink.gmane.org/gmane.linux.coreos.devel/287">Kernel developer Greg Kroah-Hartman wrote</a>,</p>

<blockquote><p>Containers are not necessarily a &ldquo;security&rdquo; boundary, there are many &ldquo;leaks&rdquo; across it, and you should use it only as a way to logically partition off users/processes in a way that makes it easier to manage and maintain complex systems. The container model is quite powerful and tools like docker and systemd-nspawn provide a way to run multiple &ldquo;images&rdquo; at once in a very nice way.</p></blockquote>

<p>Containers are powerful tools for Linux administrators, but for true multi-tenant cloud installations, we need stricter isolation between tenants.</p>

<p>Containerization is not “free”. For instance, the Linux Memory Controller can slow down the kernel by as much as 15%, just by being enabled, with no users. The Memory Controller itself is complicated, but cgroups controllers, on which it depends, are also complex. The surface of change is just way too big, and the resulting implementation necessarily too complex. <a href="https://plus.google.com/109487070944143253361/posts/6AdwyTfPFQe">George Dunlap said it best</a>,</p>

<blockquote><p>With containers you&rsquo;re starting with everything open and then going around trying to close all the holes; if you miss even a single one, bam, you lose. With VMs, you start with almost everything closed, and selectively open things up; that makes a big difference.</p></blockquote>

<p><strong>This is part 2 of a 3-part series.</strong> Please subscribe to our <a href="http://osv.io/blog/atom.xml">feed</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> to get a notification when part 3 is available.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hypervisors Are Dead, Long Live the Hypervisor (Part 1)]]></title>
    <link href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1/"/>
    <updated>2014-06-19T13:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-1</id>
    <content type="html"><![CDATA[<p><strong>By Dor Laor and Avi Kivity</strong></p>

<p>The hypervisor is the basic building block of cloud computing; hypervisors drive the software-defined data center revolution, and two-thirds of all new servers are virtualized today. Hypervisors for commodity hardware have been the key enabler for the software revolution we have been experiencing.</p>

<p>However, for the past 8 years a parallel technology has been growing, namely, containers. Recently containers have been getting a fairly large amount of traction with the development of the Docker project. When run on bare metal, containers perform better than hypervisors and have a lower footprint.</p>

<p>There is a lot in common with the goals of these technologies. These three blog entries will try to provide an answer to the question</p>

<p><strong>Will containers kill the hypervisor?</strong></p>

<p>The series will provide in-depth explanations about the underlying technology and the pros and cons of each solution.</p>

<h2>Intro: ancient hypervisor lore</h2>

<p>What is virtualization anyway?</p>

<p><img src="http://osv.io/blog/images/virtualization.png" alt="Virtualization diagram" /></p>

<p>A hypervisor is a software component (potentially assisted by hardware) that allows us to run multiple operating systems on the same physical machine. The overlay OS is called the guest OS or simply, a Virtual Machine (VM). The guest OS may not even be aware it is running on virtual hardware.</p>

<p>The interface between the guest and the host is the hardware specification. It covers the CPU itself and any other hardware devices, from BIOS to NICs, SCSI adapters, GPUs and memory.</p>

<p><img src="http://osv.io/blog/images/IBM360-67AtUmichWithMikeAlexander.jpg" alt="IBM System/360" /></p>

<p><strong>IBM, together with MIT and the University of Michigan, pioneered hypervisor technology on System/360 and System/370 mainframes, beginning in the 1960s.</strong></p>

<p>IBM was the first company to produce hypervisors. The IBM System/360, model 1967, was the first to <a href="http://www.beagle-ears.com/lars/engineer/comphist/ibm360.htm#gener">ship with virtual memory hardware supporting virtualization</a>. The next system in the series, System/370, was the “private cloud” of its day. Administrators could set up virtual machines for running different OS versions, and even public-cloud-like “time sharing” by multiple customers.</p>

<h2>Virtualization for x86</h2>

<p>Virtualization didn’t make it to commodity systems until the <a href="http://www.vmware.com/company/news/mediaresource/milestones">release of VMware Workstation in 1999</a>. In the early 2000s, hypervisors were based on pure software and were mostly useful for development and testing.  VMware initially used a technique called dynamic translation to intercept privileged operations by the guest operating system. When the guest accessed “hardware”, VMWare rewrote the instructions on the fly, to protect itself from the guest and isolate guests from each other.</p>

<p><img src="http://osv.io/blog/images/vmware-logo.png" alt="VMware logo" /></p>

<p>Later on, the open source Xen hypervisor project coined the term paravirtualization (PV). PV guests, which have to be specially modified to run on a PV host, do not execute privileged instructions themselves but ask the hypervisor to do it on their behalf.</p>

<p><img src="http://osv.io/blog/images/xen-logo.png" alt="Xen logo" /></p>

<p>Eventually, Intel, AMD and ARM implemented support for virtualization extensions. A special host mode allows running guest code on the bare CPU, getting near 100% of bare metal throughput for CPU-intensive workloads. In parallel, the memory management and the IO path received attention as well with technologies such as nested paging (virtual memory), virtual interrupt controllers, single-root I/O virtualization (SRIOV) and other optimizations.</p>

<h2>Hardware support for hypervisors</h2>

<p>Hypervisor enablement continues to be a priority for hardware manufacturers. <a href="https://plus.google.com/+OsvIo/posts/fgzsepcScTa">Glauber Costa wrote</a>, “the silicon keeps getting better and better at taking complexity away from software and hiding somewhere else.”</p>

<p><a href="http://www.redhat.com/rhecm/rest-rhecm/jcr/repository/collaboration/jcr:system/jcr:versionStorage/5e7884ed7f00000102c317385572f1b1/1/jcr:frozenNode/rh:pdfFile.pdf">According to a paper from Red Hat Software</a>,</p>

<blockquote><p>Both Intel and AMD continue to add new features to hardware to improve performance for virtualization. In so doing, they offload more features from the hypervisor into “the silicon” to provide improved performance and a more robust platform&hellip;.These features allow virtual machines to achieve the same I/O performance as bare metal systems.</p></blockquote>

<h2>Old-school hypervisors</h2>

<p>Hypervisors are one of the main pillars of the IT market (try making your way through downtown San Francisco during VMworld) and solve an important piece of the problem. Today the hypervisor layer is commoditized, users can choose any hypervisor they wish when they deploy Open Stack or similar solutions.</p>

<p>Hypervisors are a mature technology with a rich set of tools and features ranging from live migration, cpu hotplug, software defined networking and other new coined terms that describe the virtualization of the data center.</p>

<p>However, in order to virtualize your workload, one must deploy a full fledged guest operating system onto every VM instance. This new layer is a burden in terms of management and in terms of performance overhead. We’ll look at one of the other approaches to compartmentalization next time: containers.</p>

<p><strong>This is part 1 of a 3-part series.</strong>  <a href="http://osv.io/blog/blog/2014/06/19/containers-hypervisors-part-2/">Part 2 is now available</a>.  Please subscribe to our <a href="http://osv.io/blog/atom.xml">feed</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> to get a notification of future posts.</p>

<p>Photo credit, IBM 360: <a href="http://commons.wikimedia.org/wiki/File:IBM360-67AtUmichWithMikeAlexander.jpg">Dave Mills for Wikimedia Commons</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nadav Har’El Presenting OSv at USENIX June 19th]]></title>
    <link href="http://osv.io/blog/blog/2014/06/18/usenix-atc/"/>
    <updated>2014-06-18T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/18/usenix-atc</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>If you&rsquo;re attending the USENIX Annual Technical Conference, be sure not to miss
<a href="https://www.usenix.org/conference/atc14/technical-sessions/presentation/kivity">&ldquo;OSv—Optimizing the Operating System for Virtual Machines&rdquo; </a>. Nadav Har&#8217;El will be presenting tomorrow at 10:40 AM.</p>

<p>Here&rsquo;s a quick preview of some of the performance results that Nadav will show:</p>

<p><img src="http://osv.io/blog/images/usenix-paper.png" alt="table from upcoming paper" /></p>

<p>It&rsquo;s also your opportunity to ask Nadav some hard questions.</p>

<p>If you&rsquo;re not at USENIX this year, you can still get a copy of the paper.</p>

<p>Thanks to the USENIX open access policy, the paper is scheduled to go Open Access on the day of the event. To get an alert from us when it&rsquo;s up, please follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Capstan With Local OSv Images]]></title>
    <link href="http://osv.io/blog/blog/2014/06/06/capstan-push/"/>
    <updated>2014-06-06T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/06/06/capstan-push</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>If you&rsquo;re building OSv from source, you can use the <code>capstan push</code> command to temporarily use a local build in place of a base image from a network repository.  This is handy when you&rsquo;re trying your application with a patched version of OSv.   Just run <code>capstan push</code> after the OSv build to push your newly built image into your local Capstan repository.</p>

<p>For example, if your Capstanfile uses the <code>cloudius/osv-base</code> base image:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>make 
</span><span class='line'>capstan push cloudius/osv-base  build/release/usr.img</span></code></pre></td></tr></table></div></figure>


<p>When you&rsquo;re ready to go back to using the image from the network, you can run</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>capstan pull cloudius/osv-base</span></code></pre></td></tr></table></div></figure>


<p>to replace the image in your local repository with the image from the network repository.</p>

<p>For more tips and updates, please follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OSv Paper Coming to USENIX in June]]></title>
    <link href="http://osv.io/blog/blog/2014/05/19/usenix-atc/"/>
    <updated>2014-05-19T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/05/19/usenix-atc</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>We&rsquo;re going to the USENIX Annual Technical Conference
in Philadelphia!</p>

<p><a href="https://www.usenix.org/conference/fcw14"><img src="https://www.usenix.org/sites/default/files/fcw14_banner_450x93.png" border="0" alt="2014 Federated Conferences Week"></a></p>

<p><a href="https://www.usenix.org/conference/atc14/technical-sessions/presentation/kivity">Our paper, &ldquo;OSv—Optimizing the Operating System for Virtual Machines&rdquo; </a> has been accepted by one of our favorite IT events.  We appreciate all the excellent comments and questions from our peer reviewers.</p>

<p>This year, ATC will be part of a Federated Conferences Week that includes HotCloud, HotStorage, two days of sysadmin training, and more, so there should be something for everyone.</p>

<p>The paper will be available under Open Access terms starting on the date of the event, but we all hope you can come see us live and in person.</p>

<p>Here&rsquo;s the abstract:</p>

<blockquote>
<p>Virtual machines in the cloud typically run existing general-purpose operating systems such as Linux. We notice that the cloud’s hypervisor already provides some features, such as isolation and hardware abstraction, which are duplicated by traditional operating systems, and that this duplication comes at a cost.</p>

<p>We present the design and implementation of OSv, a new guest operating system designed specifically for running a single application on a virtual machine in the cloud. It addresses the duplication issues by using a low-overhead library-OS-like design. It runs existing applications written for Linux, as well as new applications written for OSv . We demonstrate that OSv is able to efficiently run a variety of existing applications. We demonstrate its sub-second boot time, small OS image and how it makes more memory available to the application. For unmodified network-intensive applications, we demonstrate up to 25% increase in throughput and 47% decrease in latency. By using non-POSIX network APIs, we can further improve performance and demonstrate a 290% increase in Memcached throughput.</p>
</blockquote>


<p>For more event updates, please follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems on Twitter</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interview: OSv on 64-bit ARM Systems]]></title>
    <link href="http://osv.io/blog/blog/2014/05/12/osv-on-64-bit-arm/"/>
    <updated>2014-05-12T20:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/05/12/osv-on-64-bit-arm</id>
    <content type="html"><![CDATA[<h2>Q&amp;A with Paul Mundt,  Jani Kokkonen, and Claudio Fontana</h2>

<p>Paul Mundt is CTO of OS &amp; Virtualization at Huawei, while Jani and Claudio are both Virtualization Architects on Huawei&rsquo;s virtualization team. All are based in Munich, which is the headquarters for Huawei’s European Research Center. The company also has a team of OSv developers in Hangzhou, China, who are focused on adaptation of OSv to Huawei&rsquo;s x86-based enterprise servers.</p>

<p><strong>Q: ARM processors are everywhere.  What are the important differences
between the Aarch64 hardware that you&rsquo;re targeting with the OSv port and
the garden-variety ARM processors that we have in our phones, toasters,
and Raspberry Pis?</strong></p>

<p>Other than the relatively obvious architectural differences in going from a 32-bit to a 64-bit architecture (more general purpose registers, address space, etc), there are quite a number of fundamental changes in v8 that considerably clean up the architecture in contrast to earlier versions.</p>

<p>One of the more substantial changes is the new exception and privilege model, with 4 exception levels now taking the place of v7&rsquo;s assortment of processor modes. The new privilege levels are much more in line with conventional CPU privilege rings (eg, x86), even though for whatever reason the numbering has been inverted &mdash; now with EL3 being the most privileged, and EL0 being the least.</p>

<p>Of specific relevance to the OSv port, through its heavy use of C++11/C1x atomic operations and memory model, are the improvements to the CPU&rsquo;s own memory and concurrency model. In contrast to x86, v7 and earlier adopt a weak memory model for better energy efficiency, but have always been terrible at sequentially consistent (SC) atomics as a result. In v8, the weak memory model has been retained, but special attention has also been paid to improving the deficiencies in SC atomics, resulting in the addition of load-acquire/store-release instruction pairs that work across the entire spectrum of general purpose and exclusive loads/stores. This places the architecture in direct alignment with the emerging standardization occurring in C++11/C1x, and has simplified much of the porting work in this area.</p>

<p>Beyond this (while not strictly v8-specific) there are also a new range of virtualization extensions to the interrupt controller that we can take advantage of, but unfortunately this part of the IP is not yet finalized and remains under NDA.</p>

<p>As our semiconductor company (HiSilicon) produces its own Aarch64 CPUs, we have also made a number of our own changes to the microarchitecture to better fit our workloads, especially in the areas of the cache and virtual memory system architecture, virtualization extensions, interconnects, and so on.</p>

<p><strong>Q: What class of applications is your team planning to run on OSv?</strong></p>

<p>We see many different potential applications for OSv within Huawei. While OSv is primarily touted as a lightweight cloud OS, the area that is more interesting for my team is its potential as a lightweight kernel for running individual applications directly on the hypervisor, as well as its ability to be used as an I/O or compute node kernel in the dataplane through virtio.</p>

<p>Tight coupling of the JVM to the hypervisor is also an area that we are interested in, particularly as we look to new directions in heterogeneous computing emerging through OpenJDK Sumatra, Aparapi, and the on-going work by the HSA Foundation in which we are also engaged.</p>

<p>Over the next year or so we also expect to see the JVM support maturing, to the point where it should also become possible to run some of the heavier weight big data stacks, but there is a long way to go first.</p>

<p><strong>Q:  When you&rsquo;re considering using OSv as a lightweight kernel for running applications directly on the hypervisor, are you considering using it  without a local filesystem?  (I understand OSv can boot in about 1/10th the time without ZFS.)</strong></p>

<p>ZFS is indeed quite heavyweight for our needs, and indeed, up until this stage in the porting effort we have largely been able to avoid it, but this will obviously not be the long-term case as we look to a solution we can bring to our customers.</p>

<p>In addition to the boot time issues you have mentioned, the ZFS adaptive replacement cache (ARC) and page cache interactivity problems with large mmap()&rsquo;s is an area of concern for some of our potential users, so this is something that we are also closely monitoring, particularly as we think about other ways we might utilize OSv for other applications in the future.</p>

<p>That being said, at the moment we basically see a few different directions to go on the file system side (and by extension, the VFS layer) for our more immediate applications:</p>

<p>1) Simple in-memory file systems with substantially reduced functionality that we can use for scenarios like dataplane applications or I/O nodes where we need no persistent storage. In these cases as we don&rsquo;t even need to support file I/O, we will likely be carrying out some customization and optimizations in this area. This is obviously in contrast to the compute node and control plane side, which we primarily intend to run under Linux in parallel for now.</p>

<p>2) Adaptation for phase change and other non-volatile memories. OSv has a much lighter weight stack with no real legacy at the moment, so fits the role of testbed quite well in terms of experimenting with the best way to tie these technologies in for different deployment scenarios, particularly under a layer of virtualization. In the long run we would naturally expect the results of this work to transfer to the Linux kernel, too.</p>

<p>3) Global and distributed filesystems &mdash; initially across OSv instances, and then across to Linux. This also has implications for the underlying transport mechanisms, particularly as we look to things like lightweight paravirtualized RDMA and inter-VM communication.</p>

<p><strong>Q:  Which hypervisor or hypervisors are you using?</strong></p>

<p>While Huawei is actively engaged across many different hypervisors, as my department (in which most of us have a Linux kernel development background) is quite focused on working close to the metal and on performance related issues, KVM is our primary focus.</p>

<p>We have previously done a fair bit of work with guest OS real-time, inter-VM communications, and I/O virtualization enhancements on ARM, so continuing with KVM also makes the most sense for us and our customers.</p>

<p>As one of the main focuses for my OS team is in heterogeneous computing, we also aim to leverage and contribute to much of the work surrounding accelerator, domain processor, and heterogeneous system architecture virtualization under KVM, although much of this is tied up in various European Union framework programmes (eg, FP7-ICT, H2020) at the moment. OSv will also continue to play an important role in these areas as we move forward.</p>

<p><strong>Q: Anything else that you would like to add?</strong></p>

<p>Only that now is an exciting time to be in OSv development. The OS has a lot of potential and is still very much in its infancy, which also makes it an excellent target for trying out new technical directions. I would also encourage people who are not necessarily cloud-focused to look at the broader potential for the system, as there&rsquo;s certainly a lot of interesting development to get involved in.</p>

<h2>About</h2>

<p><img src="http://osv.io/blog/images/Paul_Mundt.jpg" alt="Paul Mundt" /></p>

<p><strong>Paul Mundt</strong> is the CTO of OS &amp; Virtualization at Huawei’s European Research Center in Munich, Germany, where he manages the Euler department (including OS &amp; Virtualization R&amp;D, as well as new CPU development and support). Paul first joined Huawei at the beginning of 2013 as the Chief Architect of Huawei’s Server OS division, responsible for overall architecture and strategy. Prior to that, as the Linux Kernel Architect at Renesas in Tokyo, Paul was responsible for leading the Linux group within Renesas for seven years, establishing both the initial strategy and vision while taking the group from zero in-house support or upstream engagement to supporting hundreds of different CPUs across the entire MCU/MPU spectrum and becoming a consistent top-10 contributor to the Linux kernel, which carries on to this day. He has more than 15 years of Linux kernel development experience, across a diverse range of domains (HPC, embedded, enterprise, carrier grade), and has spent most of that time as the maintainer of various subsystems (primarily in the areas of core kernel infrastructure, CPU architectures, memory management, and file systems). He previously organized and chaired the Memory Management Working Group within the CE Linux Forum, where he advocated the convergence of Enterprise and Embedded technologies, resulting in the creation of Asymmetric NUMA, as well as early transparent superpage/large TLB adoption. He is a voting member of the OASIS Virtual I/O Device (VIRTIO) Technical Committee and the HSA Foundation.</p>

<p><img src="http://osv.io/blog/images/Jani_Kokkonen.jpg" alt="Jani Kokkonen" /></p>

<p><strong>Jani Kokkonen</strong> received his master’s degree in 2000 from the Technical University of Helsinki, Finland. He went to pursue research and development job in Nokia Networks. The work concentrated in research of different transport technologies on various radio access networks. This was followed by research and development activities on virtualization technologies on 3GPP radio and core network elements. Work consisted also evaluation of hardware extensions for virtualization support on various embedded multicore chips. He has been as virtualization architect in Huawei ERC Euler Department since September 2011. The work in Huawei has concentrated on research and development on QEMU/KVM on ARM and Intel platforms varying from CPU to network technologies, with his most recent effort focusing on ARM64 memory management where he is responsible for the MMU backend in the Aarch64 OSv port, as well as leading the OSv team. He is a member of the OASIS Virtual I/O Device (VIRTIO) Technical Committee and the Multicore Association.</p>

<p><img src="http://osv.io/blog/images/Claudio_Fontana.jpg" alt="Claudio Fontana" /></p>

<p><strong>Claudio Fontana</strong> received his Laurea in Computer Science in 2005 at the University of Trento, Italy, discussing a thesis on frameworks for the evaluation of taxonomy matching algorithms. He went on to pursue a software engineering opportunity in Zurich, where he worked on medium-scale (100s hosts) distributed systems. This was followed by a software engineering position in Amsterdam, working on messaging, routing, firewalls and billing systems. He has been with Huawei since December 2011. He is currently working in the virtualization area (Linux/KVM/QEMU). He is part of the early enablement effort for the ARM 64bit architecture (ARMv8 AArch64), and has been a maintainer and contributor of Free and Open Source projects, lately involving mostly QEMU binary translation (as QEMU Aarch64 TCG maintainer) and the OSv Operating System (as Aarch64 maintainer). He also spent some time as a member of Linaro&rsquo;s Virtualization team, where he focused on early Aarch64 enablement.</p>

<h2>For more information</h2>

<p>To keep up with the progress of OSv on ARM (and x86_64 too), join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bridged Networking With Capstan]]></title>
    <link href="http://osv.io/blog/blog/2014/05/07/capstan-bridge/"/>
    <updated>2014-05-07T08:37:05-07:00</updated>
    <id>http://osv.io/blog/blog/2014/05/07/capstan-bridge</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>New versions of <a href="https://github.com/cloudius-systems/capstan">Capstan</a> are making it simpler to run OSv virtual machines in a production configuration, by adding more control of network options.  A useful new feature, which helps deal with the <a href="https://github.com/cloudius-systems/osv/wiki/Running-OSv-image-under-KVM-QEMU">details of bringing up networking</a>, is the <code>-n</code> option.</p>

<p>By default, Capstan starts up KVM/QEMU with user networking:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> -netdev user,id=un0,net=192.168.122.0/24,host=192.168.122.1</span></code></pre></td></tr></table></div></figure>


<p>(That&rsquo;s from <code>ps ax | grep qemu</code>, which you can run
to see the qemu-system-x86_64 command that Capstan
is executing for you.)</p>

<p>But there are many more <a href="http://www.linux-kvm.org/page/Networking">networking options</a> for QEMU/KVM.  The basic user networking, which does not require root access to start up, is good for development and simple tasks.  But for production use, where you need to get your VM on a network where it&rsquo;s available from other VMs or from the outside, you&rsquo;ll need bridged networking.  (See your Linux distribution or hypervisor documentation for the details of creating a virtual or public bridge device.)</p>

<p>If you invoke <code>capstan run</code> with the <code>-n bridge</code> option, you&rsquo;ll get QEMU running with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-netdev bridge,id=hn0,br=virbr0,helper=/usr/libexec/qemu-bridge-helper</span></code></pre></td></tr></table></div></figure>


<p>If you have a specific bridge device to connect to, you can use the <code>-b</code> option with the name of the bridge device.  The default is <code>virbr0</code>, but you can also set up a public bridge, usually <code>br0</code>, that&rsquo;s bridged to a physical network interface on the host.</p>

<h1>Other hypervisors</h1>

<p>Don&rsquo;t feel left out if you have a different hypervisor.  Capstan also handles bridged networking on VirtualBox, with the rest of the supported hypervisors coming soon.   The fact that the syntax is the same is going to be a big time-saver for those of us who have to do testing and demos on multiple systems&mdash;no more dealing with arcane commands that are different from system to system.</p>

<p>For more on Capstan and networking, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list on Google Groups</a>.  You can get updates by subscribing to this blog&rsquo;s feed, or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New OSv Meetup Group]]></title>
    <link href="http://osv.io/blog/blog/2014/04/25/meetup/"/>
    <updated>2014-04-25T08:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/04/25/meetup</id>
    <content type="html"><![CDATA[<p><strong>By Don Marti</strong></p>

<p>We held the first meeting of the OSv <a href="http://www.meetup.com/OSv-Developer-Meetup/">Meetup group</a> in San Francisco this week, and got 14 participants from the Apache, Big Data, and OSv communities, as well as a few meetup.com users interested in cloud computing who just came along serendipitously.</p>

<p><img src="http://osv.io/blog/images/meetup.jpg" alt="attendees" /></p>

<p>Thanks to our hosts at <a href="http://ohmdata.com/">OhmData</a> who made their groovy South of Market office space available, and thanks to our attendees for coming in to try out OSv.  Looking forward to seeing the results of your initial experiments.</p>

<p>(For the users of VirtualBox on Mac OS who ran into the <a href="https://groups.google.com/forum/#!topic/osv-dev/yobHBsusLN8">&ldquo;assertion failed&rdquo; problem</a>, we&rsquo;re discussing that on the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a> now, so watch the list for an update.)</p>

<p>To get advance notice of future events&mdash;both the free-form hands-on sessions like this one and an upcoming tech talk series&mdash;please join the <a href="http://www.meetup.com/OSv-Developer-Meetup/">Meetup group</a> or follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Riemann - a Clojure Application on OSv]]></title>
    <link href="http://osv.io/blog/blog/2014/04/22/riemann-on-osv/"/>
    <updated>2014-04-22T09:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/04/22/riemann-on-osv</id>
    <content type="html"><![CDATA[<p><strong>By Tzach Livyatan</strong></p>

<p>Clojure applications run on the JVM, so they&rsquo;re usually simple to run on OSv.  We have <a href="https://github.com/tzach/capstan-example-clojure">hello world in Clojure</a> running, but this time I wanted to port a real, non-toy, Clojure application. I chose <a href="http://riemann.io">Riemann</a>, a widely-used application for aggregating system events (and more).</p>

<p>I used <a href="http://osv.io/capstan/">Capstan</a>, a tool for building and running applications on OSv.  Jump to the end <a href="https://github.com/tzach/riemann">result</a>, or follow the steps I took:</p>

<!-- more -->


<p>Following the Capstan guideline, I added a <a href="https://github.com/tzach/riemann/blob/master/Capstanfile">Capstanfile</a> to the project.  Here are the parts of Capstanfile you need to know about:</p>

<ul>
<li>Set the base image. In this case I chose a base image with Java (open-jdk)</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>base: cloudius/osv-openjdk</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Build the jar file, taking advantage of the <code>lein uberjar</code> command, which packages the application with all dependencies into one jar file.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>build: lein uberjar</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Copy the build artifacts to the base image, producing a new image:</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>files:
</span><span class='line'>  /riemann.jar: ./target/riemann-0.2.5-SNAPSHOT-standalone.jar
</span><span class='line'>  /riemann.config: ./riemann.config</span></code></pre></td></tr></table></div></figure>


<p>I also copy the config file, which Riemann will look for.</p>

<ul>
<li>The run command for the VM is executed when the VM starts.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cmdline: /java.so -jar /riemann.jar</span></code></pre></td></tr></table></div></figure>


<p>That&rsquo;s it. Done with the Capstanfile.</p>

<p><strong>Let&rsquo;s test it!</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt;capstan run
</span><span class='line'>WARN [2014-04-13 14:11:22,029] Thread-9 - riemann.core - instrumentation service caught
</span><span class='line'>java.io.IOException: Cannot run program "hostname": error=0, vfork failed
</span><span class='line'>  at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041)
</span><span class='line'>  at java.lang.Runtime.exec(Runtime.java:617)
</span><span class='line'>  at clojure.java.shell$sh.doInvoke(shell.clj:116)
</span><span class='line'>  at clojure.lang.RestFn.invoke(RestFn.java:408)</span></code></pre></td></tr></table></div></figure>


<p>No luck.  It turns out that Riemann is using</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(sh "hostname")</span></code></pre></td></tr></table></div></figure>


<p>which uses vfork to run a child process. On any OS its not very efficient to fork just to get the hostname, and on current OSv it simply won&rsquo;t work. To bypass the problem, I replace this call with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>(.getHostName (java.net.InetAddress/getLocalHost))</span></code></pre></td></tr></table></div></figure>


<p>which uses a Java <code>getHostName</code>.</p>

<p><strong>Let&rsquo;s try again</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&gt;capstan run
</span></code></pre></td></tr></table></div></figure>


<p>This time it works, but how do I test it and connect to it?</p>

<p><strong>Let&rsquo;s use Capstan port forwarding</strong></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>capstan run -f 5555:5555 -f 5556:5556
</span></code></pre></td></tr></table></div></figure>


<p>
This will  forward host ports 5555 and 5556 to the corresponding ports on the OSv VM.</p>

<p><strong>Success :)</strong></p>

<p>Now we can switch to another terminal and run:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>riemann-health
</span></code></pre></td></tr></table></div></figure>


<p>
to generate traffic for Riemann
and</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>riemann-dash
</span></code></pre></td></tr></table></div></figure>


<p>to launch a Riemann web GUI.  Here is how it looks:</p>

<p><img src="http://osv.io/blog/images/riemann_on_osv.png" alt="&quot;Riemann GUI" /> <i>riemann-dash</i></p>

<p>Now we&rsquo;re ready to do further stress testing.  If you do find any problem, or have any question, you&rsquo;re invited to join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev list</a> and ask, or post an issue to the <a href="https://github.com/tzach/riemann">GitHub repository</a>.</p>

<p>&mdash; <a href="https://twitter.com/TzachL">Tzach Livyatan</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spinlock-free OS Design for Virtualization]]></title>
    <link href="http://osv.io/blog/blog/2014/04/19/spinlock-free/"/>
    <updated>2014-04-19T09:00:00-07:00</updated>
    <id>http://osv.io/blog/blog/2014/04/19/spinlock-free</id>
    <content type="html"><![CDATA[<p>Designing an OS to run specifically as a cloud guest doesn’t just mean stripping out features. There are some other important problems with running virtualized that a conventional guest OS doesn’t address.  In this post we&rsquo;ll cover one of them.</p>

<h2>Little spinlocks, big problem</h2>

<p>In any situation where code running on multiple CPUs might read or write the  same data, typical SMP operating systems use spinlocks. One CPU acquires the lock using an atomic test-and-set operation, and other CPUs that need the data must execute a busy-loop until they can acquire the lock themselves. Can I have the data? No. Can I have the data? No. Can I have the data? No. When an OS runs on bare hardware, a spinlock might just waste a little electricity. OS developers often use other more sophisticated locking techniques where they can, and try to reserve spinlocks for short-term locking of critical items.</p>

<p><img src="http://osv.io/blog/images/apachecon.jpg" alt="OSv hacking at Apachecon 2014" /> <i>Getting some high-performance web applications running on OSv at ApacheCON 2014</i></p>

<p>The problem comes in when you add virtualization. A physical CPU that holds a spinlock is actually working. The other CPUs in the system, “spinning” away waiting for the lock, are at least waiting for something that’s actually in progress. On a well-designed OS, the lock holder will be done quickly. When the OS is running under virtualization, though, it’s another story. The hypervisor might pause a virtual CPU at times when the guest OS can’t predict. As <a href="http://www.betriebssysteme.org/Aktivitaeten/Treffen/2008-Garching/Programm/docs/Abstract_Friebel.pdf">Thomas Friebel and Sebastian Biemueller described</a> (PDF) in “How to Deal with Lock Holder Preemption”,</p>

<blockquote><p>Lock holder preemption describes the situation when a VCPU is preempted inside the guest kernel while holding a spinlock. As this lock stays acquired during the preemption any other VCPUs of the same guest trying to acquire this lock will have to wait until the VCPU is executed again and releases the lock. Lock holder preemption is possible if two or more VCPUs run on a single CPU concurrently. And the more VCPUs of a guest are running in parallel the more VCPUs have to wait if trying to acquire a preempted lock. And as spinlocks imply active waiting the CPU time of waiting VCPUs is simply wasted.</p></blockquote>

<p>If the hypervisor pauses a virtual CPU while that VCPU holds a spinlock, you get into the bad situation where other virtual CPUs on your guest are just spinning, and it’s possible that no useful work is getting done in that guest&mdash;just electricity wasting. Friebel and Biemueller describe a solution to the problem involving a hypercall to complain about the wait. But the OSv solution to the problem is to remove spinlocks from the guest OS entirely.</p>

<h2>Why going spinlock-free matters</h2>

<p>As a first step, OSv does almost all of its kernel-level work in threads. Threads, which are allowed to sleep, can use lock-based algorithms. They use mutexes, not spinlocks, to protect shared data. The <a href="https://github.com/cloudius-systems/osv/blob/master/include/lockfree/mutex.hh">mutex implementation itself</a>, however, has to use a lock-free algorithm. OSv’s <a href="https://github.com/cloudius-systems/osv/blob/master/include/lockfree/mutex.hh">mutex implementation</a> is based on a lock-free design by Gidenstam &amp; Papatriantafilou, covered in <a href="http://domino.mpi-inf.mpg.de/internet/reports.nsf/c125634c000710d0c12560400034f45a/77c097efde9fa63fc125736800444203/$FILE/MPI-I-2007-1-003.pdf">LFTHREADS: A lock-free thread library.</a> (PDF).</p>

<p>One other place that can’t run as a thread, because it has to handle the low-level switching among threads, is the scheduler. The scheduler uses per-cpu run queues, so that almost all scheduling operations do not require coordination among CPUs, and lock-free algorithms when a thread must be moved from one CPU to another.</p>

<p>Lock-free design is just one example of the kind of thing that we mean when talking about how OSv is “designed for the cloud”.  Because we can’t assume that a CPU is always running or available to run, the low-level design of the OS needs to be cloud-aware to prevent performance degradation and resource waste.</p>

<p>We’ve been posting benchmarks that show sizeable performance increases running memcached and other programs. If you’re curious about whether OSv can make your application faster, please try it out from the <a href="http://osv.io/">OSv home page</a> or join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Simple Capstan Example]]></title>
    <link href="http://osv.io/blog/blog/2014/04/03/capstan/"/>
    <updated>2014-04-03T08:37:05-07:00</updated>
    <id>http://osv.io/blog/blog/2014/04/03/capstan</id>
    <content type="html"><![CDATA[<p>(Updated 14 April 2014: Add new URL for osv-base image.)</p>

<p><a href="https://github.com/cloudius-systems/capstan">Capstan</a> is a new tool for building <a href="http://osv.io/">OSv</a> virtual machine images.  If you have worked with other tools for making VMs, you&rsquo;ll find that Capstan is really simple.  It&rsquo;s a lot like <a href="http://www.docker.io/">Docker</a> actually&mdash;only you get a complete VM out of it and not just a container.</p>

<p>You&rsquo;re probably used to blogs from sneaky tech evangelists who claim that something is simple and then post some <a href="http://drusepth.net/how-to-speed-up-your-computer-using-google-drive-as-extra-ram/">complicated set of instructions</a>.  So just to keep your finger off the close button, here&rsquo;s all you need to do.</p>

<ul>
<li><p>Add a Make target to build your application as a shared object.</p></li>
<li><p>Write a short Capstanfile.  (8 lines not counting comments).</p></li>
<li><p>Run Capstan.</p></li>
</ul>


<p>That&rsquo;s all there is to it.  Finger off the close button now?  Good.  Ready?</p>

<!-- more -->


<p>Let&rsquo;s make a VM that does something useful, say, serve this article to the entire Internet.  Go ahead and <code>git clone</code> <a href="https://github.com/cloudius-systems/capstan">Capstan</a> and follow along.</p>

<h2>An easy example, plus Makefile work</h2>

<p>Just to keep it simple, let&rsquo;s borrow the short HTTP server example from <a href="http://libevent.org/">libevent</a>.  The libevent project is a wrapper for convenient event-driven programming, and the library is used in high-profile projects such as <a href="https://www.torproject.org/">Tor</a>, the anonymous communications system, and <a href="http://www.chromium.org/Home">Chromium</a>, the basis for the Google Chrome web browser.</p>

<p>Best of all, libevent includes an easy-to-use HTTP implementation and sample code for using it.  So I&rsquo;ll copy their web server sample code, tweak it a little to make the web server I need, and set up a simple Makefile.</p>

<p>Those steps are all done in the code for this article, which is at <a href="https://github.com/dmarti/http-server">dmarti/http-server</a>.</p>

<p>You&rsquo;ll need the development package for libevent installed.  On my system, it&rsquo;s called <code>libevent-devel</code>.</p>

<p>Here&rsquo;s the target to pay attention to:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>http-server.so : http-server.c
</span><span class='line'>        $(CC) -o $@ -std=gnu99 -fPIC -shared -levent $``</span></code></pre></td></tr></table></div></figure>


<p>Yes, that&rsquo;s right, we&rsquo;re using <code>-fPIC</code> (position independent code) and <code>-shared</code> (passed to the linker, make it build a shared library).  And <code>http-server.c</code> has a function called <code>main</code>.  What&rsquo;s going on?  This is because of the way OSv works.  Your application on OSv isn&rsquo;t a conventional ELF executable, but a .so file.</p>

<p>Besides building the actual HTTP server, I&rsquo;ll also put in a Make target to create the HTML version of this article from the README, <a href="https://lwn.net/Articles/589196/">because I can</a>.  So I type <code>make</code> to build the web content and the web server.</p>

<p>Of course you can expand on this to build as complicated of an application and data set as you want.  This is just an example to show you Capstan for now.</p>

<h2>Step two: Add a Capstanfile</h2>

<p>Now it&rsquo;s time to tell Capstan how to create the virtual machine image.  Building it is easy&mdash;just run <code>make</code>&mdash;so there&rsquo;s the <code>build</code> section right there.  Now we need to tell Capstan what files go into the image, so we populate the <code>files</code> section with the name of our web server (http-server.so) the libevent shared library, and some web content&mdash;just the HTML version of this article, plus a favicon.ico file.  (For now I&rsquo;m just copying my development systems&rsquo;s copy of libevent into the image.  For real use, I&rsquo;ll come up with a more consistent way to keep track of build artifacts like this, probably borrowing them from some helpful Linux distribution.  Yes, OSv can use libraries built on and for your 64-bit Linux box.)</p>

<p>Easy so far.  Now for the <code>cmdline</code> option, which is like <a href="http://docs.docker.io/en/v0.6.3/use/builder/#cmd">Docker&rsquo;s CMD</a>: the command that gets run when the image starts.  The HTTP server just takes its DocumentRoot entry from the command line, so the command comes out as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cmdline: /tools/http-server.so /www</span></code></pre></td></tr></table></div></figure>


<p>There&rsquo;s one more section in the Capstanfile: <code>base</code>.  That&rsquo;s a pre-built OSv image, which is available from Amazon S3.  Capstan will automatically download this for you.  It lives under <code>.capstan</code> in your home directory.</p>

<h2>Putting it all together</h2>

<p>Now, when we type <code>capstan build</code>, Capstan invokes <code>make</code>, then creates the VM image.  It lives under <code>.capstan</code> in your home directory, at:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>.capstan/repository/http-server/http-server.qemu</span></code></pre></td></tr></table></div></figure>


<p>This is a QCOW2 image, ready to run under KVM or convert to your favorite format.  That&rsquo;s it.  Told you it was simple.  You can just do <code>capstan run</code> and point your browser to <a href="http://localhost:8080/">http://localhost:8080/</a> to see the site.</p>

<p>In an upcoming blog post, I&rsquo;ll cover the recently added VirtualBox support in Capstan (hint: try <code>-p vbox</code>) and some other fun things you can do.</p>

<p>If you have any Capstan questions, please join the <a href="https://groups.google.com/forum/#!forum/osv-dev">osv-dev mailing list on Google Groups</a>.  You can get updates on new OSv and Capstan progress by subscribing to this blog or folllowing <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> on Twitter.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New OSv Blog]]></title>
    <link href="http://osv.io/blog/blog/2014/03/30/welcome/"/>
    <updated>2014-03-30T20:54:23-07:00</updated>
    <id>http://osv.io/blog/blog/2014/03/30/welcome</id>
    <content type="html"><![CDATA[<p>Welcome to the OSv blog.</p>

<p>A <a href="http://osv.io/blog/atom.xml">feed</a> is available, so please subscribe in your RSS reader for updates, or, if you prefer to use Twitter, you can follow <a href="https://twitter.com/CloudiusSystems">@CloudiusSystems</a> there.</p>

<p>We&rsquo;re running <a href="http://octopress.org/docs/blogging/">Octopress</a> on <a href="http://pages.github.com/">GitHub Pages</a>, so that we can use the same easy Git contribution process for the blog as for our code.</p>

<p>Watch for a simple introduction to the <a href="https://github.com/cloudius-systems/capstan">Capstan</a> devops tool, coming this week.</p>

<p>If you have any questions about the blog, you can post a comment here, or mail <a href="mailto:dmarti@cloudius-systems.com">Don Marti</a>.</p>
]]></content>
  </entry>
  
</feed>
